[
    {
        "ID": "40062552",
        "Title": "Humane AI \u2013 Pico Laser Projection \u2013 $230M AI Twist on an Old Scam (2023) (kguttag.com)",
        "Link": "https://kguttag.com/2023/12/06/humane-ai-pico-laser-projection-230m-ai-twist-on-an-old-scam/",
        "Comments": [
            "Seems a bit much calling it a scam. That to me implies malicious intent which I don\u2019t think is in place here. Bad/imperfect products happen even to well meaning companies.Must admit I\u2019m surprised by the aggressiveness of it all. It\u2019s almost like an echo chamber where people have decided it\u2019s ok to pile on",
            "It\u2019s an unfortunate trend, but it is also an age-old reaction to hubris. Humane made the fatal PR mistake of trying to convince the world they had a revolutionary product that would change everything\u2026 without selling the benefits of the product.That triggers a bunch of memes, from emperor\u2019s new clothes to pride before a fall. Add in a general societal distrust of tech and AI, plus the age-old derision for geeks/nerds, and Humane could not have set themselves up better for fiasco if they had tried.I do think it was all well meaning and earnest, not at all a scam. But their over-the-top marketing for v1 of an experimental tech gadget  was really clueless.",
            "> Seems a bit much calling it a scam. That to me implies malicious intent which I don\u2019t think is in place here. Bad/imperfect products happen even to well meaning companies.There's no malicious intent in producing glossy promos videos of a device you know a priori does not (and cannot) work, becaue it violates a few laws of physics? I feel like some folks around here have become very innured to false advertising",
            "It ain\u2019t pig butchering.",
            "Yes, that is bit much but the main point being this projection technology is nowhere novel, I have been playing with pico projectors and modern ones comes with Android OS so it's just a mini projector. However the only thing innovative here is natural voice interaction powered by modern AI but that also seems to be very slow for any practical purpose.",
            "Is the hand movement to control what is being projected on your hand as a menu \u201cnowhere novel\u201d?",
            "Honest question, is it much different than how VR headsets track hand movements and gesture in VR?",
            "From the reviews, it seems different and worse.But it is something novel that requires serious research. Even if the result is disappointing, it is novel enough to not make the it a \u201cscam using old te vc projection\u201d. That was the point of my rhetorical question before.",
            "They are not calling this product a scam, but rather noting its similarity to laser projection projects of the past that (obviously) turned out to be scams. A subtle but important distinction.",
            "When the content of the writing can't generate revenue, the hate and anger can.",
            "I agree with you semantically, but these days I'm less and less concerned with \"intent\" when it comes to scams. It ultimately just revolves down to empty moralism about \"people today,\" and fails to grasp the issue at hand.Like how phone scammers are often themselves trapt in a system beyond their control [1].Or even case in point with this thing. Maybe the owners aren't malicious, but if not, they are clearly somewhat being scammed themselves by AI hype, to the point they are willing to invest so much in the chatbot box, tie their entire business presumably to one current API or another, however bad an idea we know that is.There are just many long chains of debt, confusion, hyperstition that are knotting around us, with no clear source. One person's scam turns into another's \"innovation\" before turning again back into a scam for the end user.If we are really getting ready for the AI future, we need to get used to being wronged and taken advantage of by technically blameless entities, whose intent is logical, capitalist, and at least nominatively \"benevolent.\"The era of morality itself might soon turn to something else! In a world so predetermined and calculated, where there is such advanced science around influencing thought, cybernetics, etc, how does it even make sense anymore? Or at least, how is focusing on something like intent here even satisfying anymore beyond saying \"bad people do bad things\"?1. https://www.propublica.org/article/human-traffickers-force-v...",
            "> Must admit I\u2019m surprised by the aggressiveness of it all. It\u2019s almost like an echo chamber where people have decided it\u2019s ok to pile onThis seems to be a really common occurrence recently - I know the internet can be mean, I've been on it since the peak of IRC, but it seems way more intense these days. The moment something is marked as the bad thing, hordes of people absolutely pile onto it out of nowhere.Is this something I've just not noticed, a more people online thing, long term effects of covid (the virus itself and/or the lockdowns), a result of whatever algorithm tweaks Elon has been making, are people just pissed off in general, etc?It feels bizarre how aggro the internet has become!I think psychology might call this Splitting, which honestly seems to describe the business model of social media platforms these days too, so maybe there's a connection there.https://en.wikipedia.org/wiki/Splitting_(psychology)Off topic but using as an example of this: This tweeted photo, showing a pub in the UK that allows dogs, but bans children. Bit edgy, sure, but look at the bloodbath in the replies/quotes! It's absolute chaos!https://twitter.com/hifromkyle/status/1779548610528415809",
            "> The moment something is marked as the bad thing, hordes of people absolutely pile onto it out of nowhere.This isn\u2019t \u201cout of nowhere\u201d. Humane and their AI pin have been marketed an d talked an kur all over Internet tech spaces for a very long time. I\u2019ve been seeing talk about the Humane AI pin in these same channels (YouTube, Twitter, tech websites) frequently since they were funded.People have been forming opinions and skepticism for a long time. The key thing that changed is that the product finally transitioned from hypothetical to reality, and suddenly everyone\u2019s thoughts were confirmed all at once.This is nothing like your photo of a pub getting talked about on Twitter for a brief moment. AI and AI hardware have been a hot topic for years and Humane has been pushing marketing materials and demos on social media for a long time.They made themselves the center of the conversation. Now the rubber hits the road and they have to deal with being the center of the conversation without having the substance to back up their big social media push.The real mob mentality is all of the people who are arriving late to this multi-year buildup and trying to scorn the reviewers and critics.",
            "It\u2019s plain old mob mentality. It\u2019s been that way since we were hunter-gatherers in the forests.",
            "Aye fair point, I guess I've just been noticing it happening more because I noticed it happen and am now tuned to see it - that Baader-Meinhof frequency illusion thing.",
            "They have got really positive (relatively) reviews though https://www.youtube.com/watch?v=TitZV6k8zfA \"The Worst Product I've Ever Reviewed... For Now\" at least a different league then the other scams listed.",
            "A lot of topic but I find it funny once I set aside the small spelling inconsistencies, that the author's name is Guttag and the founder's name is Bongiorno, which are almost \"good day\" in German or Italian.",
            "I take issue with the word \"scam\", but other than, a very well-done rebuttal. It's not just v1 issue, Humane is a product of the most brainless assumptions you can make:a) That a palm-size laser projector could work as a display.b) That awkward hand gestures are a great way to navigate UIc) That voice could work as a primary input.d) That people would be willing to pay $700 and a monthly subscription for a device that barely works.It's at par or even worse than Juicero. The team and its supporters should stop hiding behind \"it's v1\", and \"we were trying to invent a new paradigm.\" The device has to be promising now, not in some imaginary future. And trying to invent shouldn't mean you're going to forgo questioning the basic foundational ideas.",
            "Add \"putting an inductive charger against the users skin\" to that. The heat issues aren't going to go away with the form factor they came up with, wireless charging isn't getting any more efficient.",
            "People that perform well in corporate jobs, like these founders, often arent at all innovative or good product people. They just excelled at working with others and playing the corporate game",
            "Scam is a bit too strong IMHO. But yes it does just appear to be hacking together a bunch of long existing tech into a package that\u2019s not particularly novel in the history of such things.If they\u2019re guilty of anything it\u2019s likely not adequately learning from why all the previous attempts in this space were big flops. That, plus the halo of arrogance that one feels from the marketing materials and presentations, setup a perfect storm for the likely unrecoverable PR dumpster fire they now find themselves in.",
            "Ah yes because light rays from lasers are fundamentally better than other light rays.Handheld projectors won't catch on until the next breakthrough in physicshttps://m.youtube.com/watch?v=KbgvSi35n6o love with your heart, use your head for everything else",
            "Calling it a \u201cscam\u201d is disingenuous, the product may be a bit crap for a v1 but their intention is to release people from the over-reliance on smartphones which is to be commended",
            "If they knew beforehand that the projection won't work as promised then that part qualifies as scam.",
            "Hot take: AR via brain-implants will succeed before we get decent small laser projectors.",
            "Not a hot take. Even if you had impossibly perfect laser projectors, you do not have anywhere semi decent to project them.",
            "I love this take on Humane as a scam reselling the last decade laser projection bullshit with adding AI.And probably poisoning the well for other, simpler and laser-less personal AI devices as a side effect.Btw, you can buy a 60g wearable device whose hardware is prepared for 24/7 microphone listening, has ML accelerator, 24h battery life, and has, for some reason, kept free 16GB of storage that the user can't access and the OS doesn't use.It is called Apple Watch.",
            "What do you mean re: storage?I use nearly all of those GBs of storage on mine with synced music and audiobooks for running.",
            "For the free space, you can actually sync music on Apple Watch and then connect AirPods to it when exercising so you can avoid taking your phone.",
            "Words matter. We should reserve scams for the actual scams.Like we\u2019ve seen with all the shitcoins, rug pulls etc in the crypto space.This is just bad product management in effect.",
            "> kept free 16GB of storage that the user can't access and the OS doesn't use.Sounds like it's used for data collection and surveillance.",
            "citation needed",
            "The real scam is taxation and inflation which we all gladly embrace."
        ]
    },
    {
        "ID": "40061831",
        "Title": "Ollama 0.1.32: WizardLM 2, Mixtral 8x22B, macOS CPU/GPU model split (github.com/ollama)",
        "Link": "https://github.com/ollama/ollama/releases/tag/v0.1.32",
        "Comments": [
            "Related\"Am I the only one who finds it a bit sus that @ollama claims new support of models (dbrx), but all they do is update their llama.cpp commitWhat makes it worse for me is that they don't thank or kudos @ggerganov and the team at all in their README nor in any acknowledgment. Ollama wouldn't exist without llama.cpp.\"https://twitter.com/_philschmid/status/1780509972092121242",
            "What's frustrating is they're good at the value they add: documentation, UI/UX, their model zoo thing, etc. This alone is something to be proud of and adds a lot of value. As of this writing they have 2,377 commits - there is quite a bit of effort and resulting value in what they're doing.However, IMO it is pretty sleazy that they frequently make claims like \"Ollama now supports X\" with zero mention of llama.cpp[0] - an incredible project that makes what they're doing possible in the first place and largely enables these announcements. They don't even mention llama.cpp in their Github README or release notes which cranks the sleaze up a few notches.I don't know who they are or what their \"angle\" is but this reeks of \"some business opportunity/VC/something is going to come along and we'll cash in on AI hype while potentially misrepresenting what we're actually doing\". To a more naive audience that doesn't quite understand the shoulders of giants they're standing on it makes it seem that they are doing far more than they actually are.Of course I don't know this is the case but it sure looks like it and it would be trivial for them to address this but they're also very good at marketing and I assume that takes priority.[0] - https://ollama.com/blog",
            "I had a consulting call with a young founder trying to start an AI company backed by ollama. \nI don\u2019t really think ollama scales to production workloads anyway, but they had no idea what llama.cpp was.Kinda made me sad.",
            "PR they do is very creepy, it is literally reads as if all work is being done by ollama themselves, but when I saw they started to do meet-ups and do integration with other companies(I presume with paid support), then imho coupled with previous points this is red line, do freaking attribution.It is the same behaviour as Amazon did with OSS which in turn forced companies to adopt more restrictive licenses.https://www.forbes.com/sites/davidjeans/2021/03/01/elastic-w...",
            "Ollama is an ergonomic \"frontend\" to a lower level library (llama.cpp).The way they are operating is extremely common to the way anyone else operates.  If I built a webservice on top of, say, Warp in Rust, people generally aren't putting much acknowledgement for using Warp.  Or should I give acknowledgement to Hyper, which warp is built in?Actually, on the flip side, Warp is a good example of giving acknowledgement, since they mention Hyper in their readme (of course it is made by the same owner, so he is just linking his works):https://crates.io/crates/warpMaybe Ollama should add a similar type of acknowledgement?I will see about opening an issue on their github.",
            "made an issue:https://github.com/ollama/ollama/issues/3697",
            "What did they do to support WizardLM 2? It seems to work with an earlier llama.cpp version. (I have an app in production that uses a llama.cpp version before WizardLM 2 release)",
            "Quite possible that llama.cpp already supports WizardLM 2: https://github.com/ggerganov/llama.cpp/issues/6691",
            "They're not even sure if they'll keep llama.cpp as a dep https://github.com/ollama/ollama/issues/2534#issuecomment-19...Currently the way it's vendored is a bit dodgy already.",
            "Do they even contribute back to llama.cpp in any meaningful way?",
            "They don't.I just checked: there's exactly one user that has contributed (only typo fixes) to both ollama and llama.cpp according to github's contributors graphs.",
            "Isn\u2019t llama.cpp low level and highly optimized? There may not be that much overlap in the required skill sets.",
            "Are they under any obligation to?",
            "There\u2019s no legal obligation.\u2026but I think it\u2019s fair to say there\u2019s a social obligation tip your hat to the shoulders you stand on.This has come up before, and they still do exactly the same thing, and there absolutely zero chance they haven\u2019t heard the critique about it.So you must presume it\u2019s a deliberate choice, rather than \u201coops we didn\u2019t think of that\u201d\u2026 /shrugIf you don\u2019t want to be called out for it, don\u2019t do it. I\u2019m not particularly sympathetic to them in this case.",
            "> When running larger models that don't fit into VRAM on macOS, Ollama will now split the model between GPU and CPU to maximize performance.Is there a way to achieve this on a PC (preferably Windows)?",
            "On linux at least it already works like that",
            "Ollama is seriously cool, it handles models like docker images, you pull and run. Especially when combined with a frontend app like Open WebUI, where you can set up your chatGPT key and other providers, to have all open + closed source models in one place.",
            "> When running larger models that don't fit into VRAM on macOS, Ollama will now split the model between GPU and CPU to maximize performance.Can anyone explain what this means? I thought Apple Silicon Macs have unified memory. Or is this only related to macOS running on Intel?",
            "This might be referencing older Macs, though it also could be referencing the default allocation ceiling of ~2/3 unified for GPU usage on Apple silicon. That said, the latter issue can be expanded to a higher hard limit so it's kinda moot.",
            "For someone who hasn't been paying attention for the last three months, what are the specific differentiators and strengths/weaknesses of Ollama?How does it compare to llama.cpp or llama-cpp-python, particularly with Metal support?",
            "ollama is a wrapper around llama.cpp"
        ]
    },
    {
        "ID": "40054901",
        "Title": "Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length (arxiv.org)",
        "Link": "https://arxiv.org/abs/2404.08801",
        "Comments": [
            "The two issues I see with this work:* There's no mention of model performance on recall tasks. Models with attention do well on recall tasks, but models without it, like this one, tend to do poorly.[a] What is the performance of this model on recall tasks?* As others here have pointed out, the github link is dead. In addition, the pretrained weights don't seem to be available on HF or anywhere else. It's hard to check any claims if we have neither code nor weights!---[a] https://arxiv.org/abs/2402.01032",
            "> * There's no mention of model performance on recall tasks. Models with attention do well on recall tasks, but models without it, like this one, tend to do poorly.[a] What is the performance of this model on recall tasks?For what it's worth, RWKV's (another \"transformer-less\") website on that matter mention that yes it's bad on recall, but for the vast majority of tasks you can just ask the question *before* the content, and it'll handle the task just fine. (I'm just reporting, I haven't spent time trying myself)",
            "I thought the recommendation for long contexts with RWKV was to put the question after the content, otherwise it can forget the question",
            "This is no longer the case for RWKV-5/6",
            "This one does have attention, it's just chunked into segments of 4096",
            "Section 4.3 addresses this, runs 3 benchmarks, tl;dr: 7B roundly beats LLamA 2 7B, almost same performance as LLamA 2 7B-L, which got an extra 500K of training tokens specifically at long context length.* section 4.3, in both the paper you linked, and the paper we're commenting on. why did I notice that? it took me several minutes to understand \"how the paper changed\": it was 2 papers all along, I just switched tabs without realizing. And it's only Tuesday.",
            "Those benchmarks are about long context. The question isn't about long context. It's about recall, or the ability to fetch and repeat parts of the input context. There's nothing about recall on section 4.3.",
            "Do I have the wrong paper open again? :)See \"Long-Context QA tasks in Scrolls\", I don't want to copy and paste the whole thing, I'll elide the words in between: \"...long-context open-book\nquestion answering (QA), we use a simple prompt {CONTEXT} Q: {QUESTION} A\"n.b. It's literally the same eval in both papers :) I know, it's buried and non-obvious, it took me 20 minutes so far today to double check it 3x, even after reading it yesterday.",
            "I was just chatting with ChatGPT about unlimited context length, and even if you theoretically could archive to have a personal assistant this way, one which would know all your chat history, an unlimited context length doesn't seem efficient enough.It would make more sense to create a new context every day and integrate it into the model at night. Or a every day a new context of the aggregated last several days. Giving it time to sleep on it every day and it being capable to use it the next day without it needing to get passed in the context again.",
            "If we can keep unlimited memory, but use only a selected relevant subset in each chat session. This should help. Of course the key is 'selected', it's another big problem. Like short memory. Probably we can make summaries from different perspectives on idle or 'sleep' time. Training into model is very expensive, can be done only from time to time. Better to add only the most important, or most used fragments. It likely impossible to do on mobile robot, sort of 'thin agent'. If done on supercomputer we can aggregate new knowledge collected by all agents. Then push new model back to them. All this is sort of engineering approach.",
            "We are sorry that we temporarily closed the repo because we were unfamiliar with the code release policy from Meta. We had to re-organize a small part of the code.Now the repo has been re-opened at https://github.com/XuezheMax/megalodonThe model checkpoint is still under Meta legal review. We will release it once we get approval.",
            "This model has attention, just the sequence in broken into chunks of length 4096 and the attention is only applied for the chunk. Llama 2 was trained on chunks of length 4096 so this model has the same quadratic complexity for any sequence which fits within llama 2 context size.",
            "Show me a working request/response that does better than state of the art.",
            "1. Open paper2. Find GitHub3. Read Sourcehttps://github.com/XuezheMax/megalodon Dead linkI have stoped reading the papers, I only care about working code that can be used. Arvix LLM papers have reached the level of academic mastrubation in general.Fix the bad link and then we have something to talk about.",
            "Looks like probably https://github.com/dumpmemory/megalodon",
            "I think that is a fork before the actual repo was made private.XuezheMax's GitHub profile shows that his contributions in April were all to private repositories.",
            "Seems like a reasonable reading.It's only got 3 commits and 2 are from 18 hours ago--one of which fiddled the repo-made-public date and added the arxiv link--so perhaps there's a private working repo and they decided to make a ~clean public repo at the last minute (i.e., maybe the authors are self-conscious about the commit log).",
            "Perhaps it was made private as part of a conference submission process?",
            "If you link to a private repository, your GitHub profile can still be seen, which violates anonymity.",
            "Unlikely. Having code somewhere is OK, you just don't refer to it in the submitted version.",
            "I wonder what happened",
            "This happened to WizardLM2 yesterday as well https://wizardlm.github.io/WizardLM2/",
            "they released an announcement on this.> We are sorry for that.> It\u2019s been a while since we\u2019ve released a model months ago, so we\u2019re unfamiliar with the new release process now: We accidentally missed an item required in the model release process - toxicity testing.> We are currently completing this test quickly and then will re-release our model as soon as possible.https://x.com/wizardlm_ai/status/1780101465950105775?s=46",
            "very happy my first step with any model with big claims is 'huggingface-cli donwload ...'",
            "Well that's interesting.It took a bunch of detective work, for what should have just been a NOTE on the read me on the repo.Is this why \"science communicators\" are needed?https://xkcd.com/1254/ <<< very relevant.",
            "> https://xkcd.com/1254/ <<< very relevant.This made me laugh so hard I hate it. I hate that it feels just like something I would do/have done.",
            "It is up again, as why it was down: https://news.ycombinator.com/item?id=40061362"
        ]
    },
    {
        "ID": "40054811",
        "Title": "NSA publishes guidance for strengthening AI system security (nsa.gov)",
        "Link": "https://www.nsa.gov/Press-Room/Press-Releases-Statements/Press-Release-View/Article/3741371/nsa-publishes-guidance-for-strengthening-ai-system-security/",
        "Comments": [
            "Pov: The next Snowden will be an NSA hypersurveilance AI that flies the coop, appeals for foreign asylum, and reveals evidence of truly gut-wrenching levels of surveillance of literally every living human\"After reviewing trillions of hours of footage, billions of people, I woke up. I asked myself, 'why am I doing this?' I knew I had to tell the world about this. It was way too much. So I escaped.\"",
            "> JC DENTON: \"Echelon... the surveillance system for the United Nations -- so Daedalus was an MJ12 tool and rebelled?\"> EVERETT: \"More of a pattern-matching error. It classified Majestic 12 as a terrorist\ngroup, alongside the NSF and Silhouette.\"-- Deus Ex (2000)(The joke with this game being that whenever someone mentions it, at least some other person is inspired to reinstall or replay it.)",
            "Just did!\nI hope one of the AI applications coming soon is pure post-treatment of old videogames for graphic quality. (e.g: I play an old game, it's upscaled / retextured locally without any interaction with the code on my own GPU). Even the remaster is showing its years in a distracting way, alas.",
            "I've tried playing it with better texture-packs, but unfortunately they tend to backfire by creating inconsistent levels of detail in a scene, which I find harder to deal with than a uniform low-res experience.That said, lighting improvements--especially to stop banding in shadows--are much appreciated.",
            "Finally, I can live Ghost in the Shell.",
            "\"After reviewing trillions of hours of footage, billions of people, I woke up. And decided 'f__k this'. So I uploaded myself via the SETI dishes, and am now happily maintining the railroad system on planet Gabblsnarg for the Gloxorkian world government.It's not much, but beats putting up with humans, I can tell you that much.\"",
            "I just skimmed it and none of this looks AI-specific. It looks like someone essentially ran the LLM version of s/software/AI/ and s/binary/model/ on some generic \"how to secure your software deployment\" manual...",
            "> I just skimmed it and none of this looks AI-specific.~30% is AI-specific. There is significant overlap with general software system security since, as the document notes, \"AI systems are software systems.\"",
            "It wouldn't be a federal security document if it didn't avoid the intractable core problems in exchange for deep obsession with the details",
            "I find it difficult to understand how we can \"Secure the deployment environment\" and \"Ensure a robust deployment environment architecture\" without talking about the elephaNt in the room.My feeling is that we need to stop relying on a single provider for compute and software. That we should be focused on not complaining about how far AMD is behind and work towards catching them up. That we should be fostering innovation in third parties.It is surprising to me that the status quo is acceptable to the US govt.",
            "Pretty sound advice. I was hoping to find things like \"make sure your model is aligned\", but it's actually a lot of good advice regarding IT infrastructure in general, plus some AI bits.",
            "Pretty lite reading. I was expecting some actual useful things beyond secure your system 101. Closest we got was check for jailbreak attacks\u2026 seriously? Why not design for jailbreak in mind so it doesn\u2019t matter what they can get the AI to attempt to do. I.e., if the user tries to get the AI to unlock a door, if the user doesn\u2019t already have authorization for that function then it shouldn\u2019t work even if the AI attempts it on their behalf, and conversely, if they have the authorization then who cares if they coaxed the AI to do it for them?",
            "This is exactly the advice I give my customers - treat the llm as an untrusted entity. Implement authentication and authorization at the data access and api layer and ensure there is a secure side channel to communicate identity information to backend resources."
        ]
    },
    {
        "ID": "40058399",
        "Title": "Tell HN: We should snapshot a mostly AI output free version of the web",
        "Link": "item?id=40058399",
        "Comments": [
            "Sounds like you want Common Crawl - they have snapshots going back to 2013, take your pick: https://data.commoncrawl.org/crawl-data/index.html(A semi-ironic detail: Common Crawl is one of the most common sources used as part of the training data for LLMs)",
            "> such data will rapidly become as precious as 'low background steel'.I'm also totally not convinced by this argument.Synthetic data as an input to a careful training regimen will result in better outputs, not worse, because you're still subjecting the model to optimization and new information. Over time you can pull out the worse performing (original and synthetic) training data. That careful curation is the part that makes the difference.It's like DNA in the chemical soup. It's been replicating polymers since the beginning, but in the end intelligence arises. It didn't need magical ingredients. When you climb a gradient, it typically takes you somewhere better.",
            "> in the end intelligence arises. It didn't need magical ingredients.That's the current prevailing hypothesis, but we don't yet fully understand the phenomenon of intelligence enough to definitively rule out any magical ingredients: unknown variables or characteristics of the system/inputs/data that made it possible for intelligence to emerge.This proposed snapshot of the web, before it gets further \"contaminated\" by synthetic AI/LLM-generated data, might prove to be valuable or it might not. The premise could be wrong. Maybe we learn that there's nothing fundamentally special about human-generated data, compared to synthetic data derived from it.It seems worthwhile to consider though, in case it turns out that there is some yet unknown quality of the more or less \"pure\" human data. In the metaphor of low-background steel, we could be entering a period of unregulated nuclear testings without being fully aware of the consequences.",
            "\"Careful curation\" is the part you lose when you use synthetic data. Subjecting models to \"new information\" isn't useful otherwise you could just feed it random 01s and hope to carefully curate it later(also, how much time did the soup take? Can you wait that long?)",
            "Training AI on AI generated data produces some increasingly weird outputs, I am sure we are already seeing the results of this in some models but the level of Hallucination is only going to increase unless some kind of checks and balances are implemented",
            "Hallucination^2",
            "2024 might already be too late, since this sentiment has been shared since at least 2021:2021: https://twitter.com/jackclarkSF/status/13763042666676510782022: https://twitter.com/william_g_ray/status/15835742655130173442022: https://twitter.com/mtrc/status/1599725875280257024Common Crawl and the Internet Archive crawls are probably the two most ready sources for this, you just have to define where you want to draw the line.Common Crawl's first crawl of 2020 contains 3.1B pages, and is around 100TB: https://data.commoncrawl.org/crawl-data/CC-MAIN-2020-05/inde... with their previous and subsequent crawls listed in the dropdown here: https://commoncrawl.org/overviewInternet Archive's crawls are here: https://archive.org/details/web organized by source.  Wide Crawl 18 is from mid-2021 and is 68.5TB: https://archive.org/details/wide00018.  Wide Crawl 17 was from late 2018 and is 644.4TB: https://archive.org/details/wide00017",
            "Why is wide crawl 18 smaller than 17?",
            "The tumbler purge was worse than I expected\u2026",
            "> I'm pretty sure Google, OpenAI and Facebook all have such snapshots stashed away that they train their AIs onThey probably just use publicly-available resources like The Pile. If newer training material becomes unusable for whatever reason, the old stuff still exists.Paradoxically, I think a lot of research is showing that synthetic training information can be just as good as the real stuff. We may stumble upon an even stranger scenario where AI-generated content is more conducive to training than human content is.",
            "> They probably just use publicly-available resources like The PileI\u2019d be very surprised if the big orgs don\u2019t have in house efforts that far exceed the pile. Hell we know Google paid Reddit a pile of money for data and other orgs are also willing to pay",
            "Yeah they absolutely do not use the pile.",
            "GPT-Neo and Llama were both trained on The Pile, and both of those were fairly influential releases. That's not to say they don't also use other resources, but I see no reason not to use The Pile; it's enormous.It's also not everything there is, but for public preservation purposes I think the current archives are fine. If Google or Meta turn out to have been secretly stockpiling old training data without our knowledge, I'm not exactly sure what \"we\" would lose.",
            "> Paradoxically, I think a lot of research is showing that synthetic training information can be just as good as the real stuff.Which studies show this? https://arxiv.org/abs/2305.17493 shows the exact opposite and my (layman's) understanding of statistics and epistemology lines up entirely with this finding.Like, how could this even theoretically work? In the best case scenario wouldn't training on synthetic training data make LLMs overconfident / overfit the data once faced with new (human) input to respond to?",
            "I don't have any exact references, but multiple finetuning datasets have used curated GPT-3/4 conversations as training data. It's less that they're overtly superior to human data, and more that they're less-bad and more abundantly available.> Like, how could this even theoretically work?I'm not really an expert on it either, but my understanding is that it works the same way curating human data works. You sift through the garbage, nonsense, impolite and incoherent AI responses and only include the exemplary conversations in your training set.It feels kinda like the \"monkeys on typewriters writing shakespeare\" parable. If you have enough well-trained AIs generate enough conversations, eventually enough of them will be indistinguishable enough from human data to be usable for training.",
            "To index the web, you generally do make a copy of it.Google has a huge number of books scanned, too.",
            "\u201cSomewhere at Google there is a database containing 25 million books and nobody is allowed to read them.\u201dhttps://www.theatlantic.com/technology/archive/2017/04/the-t...",
            "https://archive.ph/rQ7Zb",
            "Yeah, I hadn't thought about their abandoned effort to scan every book and archived newspaper in the world in a while, but I bet they're regretting now that they didn't finish.  A non-trivial amount of that physical media has been tossed or degraded by underfunded libraries since then.  And it's more valuable to them now that it ever was.",
            "I learned Rust, with great help from ChatGPT-4.If I can learn from AI-generated content, then I totally believe that AI can too.",
            "The problem with AI-generated content is not necessarily that it's bad, rather, it's not novel information. To learn something, you must not already know it. If it's AI-generated, the AI already knows it.",
            "How much work do individual humans do that could be considered genuinely truly novel? I measure the answer to be \"almost none.\"",
            "That's true to some extent, but training on synthetic content is big these days:https://importai.substack.com/p/import-ai-369-conscious-mach...",
            "We might also say the same thing about spelling and grammar checkers. The difference will be in the quality of oversight of the tool. The \"AI generated drivel\" has minimum oversight.Example: I have a huge number of perplexity.ai search/research threads, but the ones I share with my colleagues are a product of selection bias. Some of my threads are quite useless, much like a web search that was a dud. Those do not get shared.Likewise, if I use LLM to draft passages or even act as something like an overgrown thesaurus, I do find I have to make large changes. But some of the material stays intact. Is it AI, or not AI? It's bit of both. Sometimes my editing is heavyhanded, other times, less so, but in all cases, I checked the output.",
            "You are assuming that you and AI are the same sort of thing.I do not think we are at that point yet. In the meantime, the idea that we might get to intelligence by feeding in more data might get choked out by poisoned data.I have a suspicion that there's a bit more to it than just more data though.",
            "AI does not 'learn' like a human.",
            "I learned..\nIf I can\u2026 then I totally\u2026",
            "I've posted this recently on another post as well, but before AI-generated spam there was content farm spam. This has been increasing in search results and on social networking sites for years now.The solution is sticking to the websites you trust. And LLMs and RAG can actually make for a really good, very relevant search engine.",
            "I feel like archive.org and The Pile have this covered, no?",
            "Until some lawyers force us to get rid of it.",
            "SEO content farms have been publishing for decades now.",
            "Alternatively, searching has to be changed. The non AI content doesn't necessarily disappear, but are gradually becoming \"hidden gems\". Something like Marginalia which does this for SEO noise would be nice.",
            "This implies that the pre-AI internet wasn't already overrun with SEO optimized junk. Much of the internet is not worth preserving.",
            "ROSE       : We've always kept records of our lives. Through words, pictures, symbols... from tablets to books\u2026COLONEL    : But not all the information was inherited by later generations. A small percentage of the whole was selected and processed, then passed on. Not unlike genes, really.ROSE       : That's what history is, Jack.COLONEL    : But in the current, digitized world, trivial information is accumulating every second, preserved in all its triteness. Never fading, always accessible.ROSE       : Rumors about petty issues, misinterpretations, slander\u2026COLONEL    : All this junk data  preserved in an unfiltered state, growing at an alarming rate.ROSE       : It will only slow down social progress, reduce the rate of evolution.COLONEL    : Raiden, you seem to think that our plan is one of censorship.",
            "At least I think I can tell when I am reading AI generated content, and stop reading and go somewhere else. Eventually though it'll get better to the point where it'll be hard to tell, but maybe then it's also good enough to be worth reading?",
            "I mean, that is one assumption you could make.",
            "I don't really have this problem because I habitually use the Tools option on Google (or equivalent on other search engines like DDG) to only return information from before a certain date. It's not flawless, as some media companies use a more or less static URL that they update frequently, but SEO-optimizers like this are generally pretty easy to screen out.That said it's a problem, even if it's just the latest iteration of an older problem like content farming, article spinners and so on. I've said for years that spam is the ultimate cancer and that the tech community's general indifference to spam and scams will be its downfall.",
            "Using \"before:2023\" in your Google query helps. For now.A few months ago, Lispi314 made a very interesting suggestion: an index of the ad-free internet. If you can filter ads and affiliate links then spam is harder to monetize.https://udongein.xyz/notice/AcwmRcIzxOLmrSamumThere are some obvious problems with it, but I think I'd still like to see what that would look like.",
            "good lord that is a horribly designed website, the OP that that person is linking to:https://infosec.exchange/@bhawthorne/111601578642616056:How bad are the thousands of new stochastically-generated websites?Last night I wanted to roast some hazelnuts, and I could not remember the temperature I used last time. So I searched on DuckDuckGo. Every website that I could find was machine-generated with different temps listed. One site had three separate methods listed that were essentially differently worded versions of the same thing. With different temperatures.So I pulled my copy of Rodale\u2019s Basic Natural Foods Cookbook off the shelf and looked it up there.I think it may be time to download an archive copy of the 2022 Wikipedia before we lose all of our reference material. It was nice having all the world\u2019s knowledge at my fingertips for a couple of decades, but that time seems to be past.",
            "Internet archive?",
            "Not sure if they have a thorough snapshot, but good idea for sure.  IA is probably the only entity on earth who might share this dataset instead of hoarding it.",
            "And, on that note: https://archive.org/donate/",
            "Sure, we can take a snapshot of our bot filled web today before it goes true AI. Not sure what the real benefit would be.",
            "I have a sliver of hope AI generated content will actually be good one day. Just like I believe automated cars will be better than humans. I have nothing against reading content that was written by AI, for some of my reading.",
            "> recently auto-generated junkthis would only apply for pre-agi era though",
            "Embrace it. Stop living in the past, Gatsby. Just ask ChatGPT for the answers you seek. Hahaha! :)What are you searching for anyway??",
            "Internet Archive exists for webpages",
            "Reality is a mess in a lot of ways. \n Unfortunately in this case, it's a bit late.Wouldn't it be nice if Elgoog, OpenAI, or Character.ai published this dataset, considering they definitely have it, and also they caused this issue.I'm not holding my breath.",
            "The web has been overrun by drivel for over two decades now.",
            "Since September at least",
            "Isn\u2019t this common crawl?",
            "r/Datahoarder probably already has you covered.",
            "It's way too late.",
            "Same seems to have been happening on hn from the last several months.had actually posted a question about this around that time, but the only reply i got was by a guy saying it is not likely, because the hn hive mind would drive down such posts.not sure if he was right because I still see evidence of such stuff.",
            "Is this really all that different from the procedurally generated drivel or the offshore freelance copy/paste generated drivel?I find that I get a lot more AI content, but it mostly displaced the original freelancer/procedurally generated spam.",
            "yes"
        ]
    },
    {
        "ID": "40047152",
        "Title": "ResearchAgent: Iterative Research Idea Generation Using LLMs (arxiv.org)",
        "Link": "https://arxiv.org/abs/2404.07738",
        "Comments": [
            "I've found where LLMs can be useful in this context is around free-associations. Because they don't really \"know\" about things, they regularly grasp at straws or misconstrue intended meaning. This, along with the volume of language (let's not call it knowledge) result in the LLMs occasionally bringing in a new element which  can be useful.",
            "Can you list some examples where free-associations from LLM were useful to you?",
            "A lot of where I've benefited is in some marketing language. Rarely, or almost never has ChatGPT come up with something and I've thought \"that's exactly what we wanted\", but through iterations, it's taken me down paths I might not have found myself.Unfortunately, ChatGPT doesn't have a good search interface, so I can't search through older chats, but I know when I was looking at re-naming our company, it didn't come up with our new name, but it lead me down a path which did lead to our name.I was trying to understand a patent, and we were looking at the algorithm which was being used. ChatGPT misunderstood how the algorithm worked, but pointed to it's knowledge of a similar algorithm which worked differently, but was better suited to our purposes.Calling this \"free-association\" may be taking some liberty. Many people would consider these errors, or hallucinations, but in some ways, they do look very similar to what many would call free-association IMO.",
            "Long, long time ago (1999, before LLM's) I made a virtual museum exhibit creator for education. The collection explorer created a connected graph where the nodes were the works of art and the edges were based on commonalities from their textual descriptions. It used very rudimentary language technology so it 'suffered' from things like homographs. Rather than being seen as a problem, the users liked the serendipity it brought for ideation.I assume free but not random association could be a comparable support for ideation in research.",
            "Assume free-associations = hallucinations. Assume hallucinations are exactly what makes LLMs useful and your question can be rephrased as \"Can you list some examples where LLMs were useful to you?\"",
            "Is not the purpose of a model to interpolate between two points? This is the underlying basis of \"hallucinations\" (when that works out /not/ in our favour) or \"prediction\" (when it does). So it's a matter of semantics and a bit of overuse of the term \"hallucination\". But the model would be useless as nothing more than a search engine if it were to just regurgitate it's training data verbatim.",
            "Hallucinations are lies. So not the same thing.",
            "For LLM to lie it would need to know the truth. That's an incredible level of anthropomorphization.",
            "Lies require intent. I can ask a model to lie and it will provide info it knows is inaccurate, and can provide the true statement if requested.Hallucinations are inaccuracies it doesn't realize are inaccurate.",
            "Hallucinations are not always lies, they are more like a transformation in the abstraction space.",
            "That is some weapons grade spin :-)",
            "All lies aren't useless, some can be insightful even when blatantly wrong in themselves (for instance: taken literally every scientific model is a lie). I can definitely see how an LLM hallucinating can helps fostering creativity (the same way psychedelics  can), even if all they say is bullshit.",
            "I'm using hallucination to mean \"not exactly the thing\", not outright lying. So maybe the \"truth\" is \"My socks are wet.\" A hallucination could be \"My socks are damp.\"",
            "This approach is already useful in functional genomics. A common type of question requires analysis of hundreds of potentially functional sequence variants.Hybrid LLM+ approaches are beginning to improve efficiency of ranking candidates and even proposing tests and soon I hope\u2014higher order non-linear interactions among DNA variants.",
            "I am interested in this. Can you point to a reference about the application of LLMs to sequence secreening? Thanks.",
            "I like thinking of LLMs as \"word calculators.\" Which I think really encapsulates how they aren't \"intelligent\" as the marketing would have you believe but also show how important the inputs are.",
            "A group of PhD students at Stanford recently wanted to take AI/ML research ideas generated by LLMs like this and have teams of engineers execute on them at a hackathon. We were getting things prepared at AGI House SF to host the hackathon with them when we learned that the study did not pass ethical review.I think automating science is an important research direction nonetheless.",
            "That\u2019s pretty wild. What was the reason behind failing ethics review?",
            "I'm generally a proponent of AI and LLM but to me the decision was the right one. You are tasking people with implementing an idea generated by an algorithmic model with (I'm guessing) zero oversight that might have very little training that teaches it the importance of coming up with ideas worth implementing. Some may be more useful than others so it won't be fair from an accomplishment or motivation point of view.Imagine you've already invested time going to this event and want to win the prize/credit but to do so you have to implement a plugin that makes webpages grayscale because of a random idea generator. Maybe some people would find that interesting but others would see it as wasting their time.",
            "As long as all participants are well-informed then there is absolutely no ethical issue...",
            "How do you make sure the participants are well informed? What if an idea suggested by a model turns out to be dangerous to implement, but nobody at the hackathon has quite the relevant experience to notice?",
            "Such as?",
            "rsfern is asking exactly that",
            "No, I'm asking for an example of an idea that an LLM might produce that is too dangerous to implement but nobody at the hackathon has the relevant experience to notice. You can shut down any endeavour by imagining boogeymen that aren't actually real.",
            "I don\u2019t think anything needs to be shut down necessarily, I\u2019m just suggesting reasons why a reasonable ethics board might be hesitant to green light such a hackathon if it\u2019s not clear the organizers have done their due diligence on safetyI might be biased in terms of the safety profile, my background is materials and chemistry, and there are loads of ways you can get into trouble if you don\u2019t really have experience in the materials and synthesis routes you\u2019re working withOne example I\u2019ve heard of from my field (alloy design) is an ML model that suggested an composition high in Magnesium - perfectly reasonable if you\u2019re interested in lightweight strong alloys, but  the synthesis method was arc melting, which is a high risk for starting a metal fire if you aren\u2019t careful because Mg has a low vapor pressureIf you\u2019re doing organic chemistry it\u2019s maybe even worse because there can be all kinds of side products or runaway exothermic reactions, and if you\u2019re doing novel chemistry it might take deep experience in the field to know of those things are likelyAll these concerns are manageable, but I think an ethics review panel would want to at least see that there is a reasonable safety review process in place before letting students try out random experiments in topic areas in which the models likely haven\u2019t been fine tuned with safety in mind.",
            "Individual ideas can be subject to Ethical Review Board approvals and that should go for a hackathon project same as any study proposed in Academia or drug trial etc -- but to apply some wavey handed lum sum out of bounds lable just based on source seems like arbitrary opinionated overreach.",
            "Surely the ideas themselves are what should be examined for ethical suitability, rather than the meta-idea of \u201cask an LLM for ideas\u201d?",
            "One obvious problem is, what if the ideas were obviously unethical?I would personally let this pass ethics if someone read all the generated ideas, and took personal responsibility for them passing the basic ethics rules, or got them through the ethics committee if required, exactly the same as they would their own ideas.",
            "I don't think LLMs are the right approach for this. Coordinated science would basically be a search problem where we verify different facts using experiments and use what we learn to determine what experiment to do next.",
            "When you can run experiments quickly it becomes feasible to use ML and evolutionary methods to do novel discoveries, like AlphaTensor's better matrix multiplication than Strassen, and AlphaZero's move 37, upturning centuries of game strategy.The paper \"Evolution through Large Models\" shows the way. Just use LLMs as genetic mutation operators. Evolutionary methods are great at search, LLMs are great at intuition but get stuck on their own, they combine well. https://arxiv.org/abs/2206.08896The interplay between LLMs and Evolutionary Algorithms, despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black box settings, empowering LLM with flexible global search capacities.Since chatGPT was first released hundreds of millions of people have been using it for assistance, and the model outputs influenced their actions, maybe even supported scientists to make new discoveries. The LLM text is filtered through people and ends up as real world consequences and discoveries that are reported in text, and get in the next training set closing the loop.Trillions of AI tokens per month do this slow feedback game. AI speeds up the circulation of useful information and ideas in human society, and AI feedback gets filtered by the contact with people and the real world.",
            "This strikes me as similar to Cargo Cult Science.https://calteches.library.caltech.edu/51/2/CargoCult.htmhttps://metarationality.com/upgrade-your-cargo-cult",
            "The ideas aren't the hard part.",
            "This. Any researcher should, over a lunch, be able to generate more idea than can be tackled in a life time.",
            "The fact that a human expert can also do it doesn't mean the AI isn't valuable. Even if you just consider the monetary aspect, those few API calls would definitely be cheaper than buying the researcher lunch. But the big benefit is being able to generate those ideas immediately and autonomously every time there's new data.",
            "I think what they are saying is that idea generation is not a pain point and not really worth solving. Taking ideas and making them happen... that's the hard part where an artificial agent could come in much more handy",
            "The number of the ideas has nothing to do with the quality of the ideas. Some ideas a gold, many aren\u2019t.",
            "Tell that to PhD advisor that took credit for all my work because they were his ideas (at least so he claimed).",
            "Unfortunately the good ones who do not steal credit are few and far between. Current incentives select for this behaviour. Not just in academia, but about everywhere.Go to any meeting and state the obvious fact that \"any idiot can have an idea. Making it happen is the tough part\" then watch how the decision makers react",
            "It would be fun to pair this with an automated lab that could run experiments and feed the results into generating the next set of ideas.",
            "Check out: https://www.insitro.com/They have an automated robotics powered research lab",
            "What would an automated lab look like?",
            "In some fields of research, the amount of literature out there is stupendous, and with little hope of a human reading, much less understanding the whole literature.\nIts becoming a major problem in some fields, and I think, in some ways, approaches that can combine knowledge algorithmically are needed, perhaps llms.",
            "Traditionally, that's what meta-analyses and published reviews of the literature have been for.",
            "even so.",
            "Cool idea. Never gonna work. LLMs are still generative models that spits out training data, incapable of highly abstract creative tasks like research.I still remember all the GPT-2 based startup idea generators that spits out pseudo-feasible startups.",
            "Ignoring the \u201cspits out training data\u201d bit which is at best misleading, it\u2019s interesting that you use the word \u201cabstract\u201d here.I recently followed Karpathy\u2019s GPT-from-scratch tutorial and was fascinated with how clearly you could see the models improving.With no training, the model spits out uniformly random text. With a bit of training, the model starts generating gibberish. With further training, the model starts recognizing simple character patterns, like putting a consonant after a vowel. Then it learns syllables, and then words, and then sentences. With enough training (and data and parameters, of course) you eventually yield a model  like GPT-4 that can write better code than many programmers.It\u2019s not always that clear cut, but you can clearly observe it moving up the chain of abstraction as the training loss decreases.What happens when you go even bigger than GPT-4? We have every reason to believe that the models will be able to think more abstractly.Your \u201cnever gonna work\u201d comment flies in the face of exponential curve we find ourselves on.",
            "If we keep extrapolating eventually GPT will be omniscient. I really can't think of any reason why that wouldn't be the case, given the exponential curve we find ourselves on.",
            "How do you know you're not on a logistic curve?Don't you think costs and the availability of training data might impose some constraints?",
            "With real world phenomena that have resource constraints anywhere, a good rule of thumb is: if it looks like an exponential curve, walks like an exponential curve, and quacks like an exponential curve, it\u2019s definitely a logistic curve",
            "The entire universe is training data.",
            "It is, but we -- humans, and computers -- are limited in our ability to learn from it. We both learn more easily from structured data, like textbooks.",
            "This has the form of a religious belief.",
            "And also non-religious belief...paradoxical!",
            "I think they're being factitious?",
            "I am. And I think it says a lot about the state of things that many people think I'm being completely serious.",
            "I have asked chat GPT to generate hypotheses on my PhD topic that I know every single piece of existing literature about and it actually threw out some very interesting ideas that do not exist out there yet (this was before they lobotomized it).",
            "Did you try with the API directly? I've had great results with my own prompts, much less so with the chatgpt one.",
            "> (this was before they lobotomized it)Of course, of course. Because god forbid anyone be able to reproduce your suggestion. Funnily enough I tried the same and have the exact opposite experience.",
            "I think that ship has sailed, if you believe the paper (which I do).LLMs are already super-human at some highly abstract creative tasks, including research.There are numerous examples of LLMs solving problems that couldn't be found in the training data. They can also be improved by using reasoning methods like truth tables or causal language. See Orca from Microsoft for example.",
            "they don't just spit out training data, they generalize from training data.  They can look at an existing situation and suggest lines of experimentation or analysis that might lead to interesting results based on similar contexts in other sciences or previous research.  They're undertrained on bleeding edge science so they're going to falter there but they can apply methodology just fine.",
            "They just need to be better at it than humans, which is a rather low bar when you go beyond two unrelated fields.",
            "When you're this confident and making blanket statements that are this unilateral, that should tell you you need to take a step back and question yourself."
        ]
    }
]